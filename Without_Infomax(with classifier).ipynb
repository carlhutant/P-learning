{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import imageio\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.backend import int_shape\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change the dataset here###\n",
    "dataset = 'AWA2'\n",
    "##############################\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_dir = './data/{}/IMG/train'.format(dataset)\n",
    "val_dir = './data/{}/IMG/val'.format(dataset)\n",
    "IMG_SHAPE = 128\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "gamma = 1.8 # 先验分布的loss比重\n",
    "alpha = 0.5 # 分類的效果\n",
    "# attr_type = 'b','c','cmm','cms'\n",
    "attr_type = 'cms'\n",
    "\n",
    "if dataset == 'SUN':\n",
    "    class_attr_shape = (102, )\n",
    "    lable_shape = (645, )\n",
    "    class_attr_dim = 102\n",
    "    class_num = 717\n",
    "    seen_class_num = 645\n",
    "    unseen_class_num = 72\n",
    "elif dataset == 'CUB':\n",
    "    class_attr_shape = (312, )\n",
    "    lable_shape = (150, )\n",
    "    class_attr_dim = 312\n",
    "    class_num = 200\n",
    "    seen_class_num = 150\n",
    "    unseen_class_num = 50\n",
    "elif dataset == 'AWA2':\n",
    "    class_attr_shape = (85, )\n",
    "    lable_shape = (40, )\n",
    "    class_attr_dim = 85\n",
    "    class_num = 50\n",
    "    seen_class_num = 40\n",
    "    unseen_class_num = 10\n",
    "elif dataset == 'plant':\n",
    "    class_attr_shape = (35, )\n",
    "    lable_shape = (25, )\n",
    "    class_attr_dim = 35\n",
    "    class_num = 38\n",
    "    seen_class_num = 25\n",
    "    unseen_class_num = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continous attr mean std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_continous_path = './data/{}/predicate-matrix-continuous.txt'.format(dataset)\n",
    "attr_list = []\n",
    "RealCE_continous = pd.read_csv(attr_continous_path,header=None,sep = '\\t')\n",
    "for idx in range(len(RealCE_continous)):\n",
    "    tmp = RealCE_continous[0][idx].split(' ')\n",
    "    attr = [float(i) for i in tmp if i!='']\n",
    "    attr = np.array(attr)\n",
    "    attr = (attr - np.mean(attr)) / np.std(attr)\n",
    "    attr_list.append(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "\n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature._keras_shape[channel_axis]\n",
    "\n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "\n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool._keras_shape[1:] == (1,1,channel)\n",
    "\n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool._keras_shape[1:] == (1,1,channel)\n",
    "\n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature._keras_shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature._keras_shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "\n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool._keras_shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool._keras_shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat._keras_shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature._keras_shape[-1] == 1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SE block V1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(block_input, num_filters, ratio=8):                             # Squeeze and excitation block\n",
    "\n",
    "    '''\n",
    "        Args:\n",
    "            block_input: input tensor to the squeeze and excitation block\n",
    "            num_filters: no. of filters/channels in block_input\n",
    "            ratio: a hyperparameter that denotes the ratio by which no. of channels will be reduced\n",
    "            \n",
    "        Returns:\n",
    "            scale: scaled tensor after getting multiplied by new channel weights\n",
    "    '''\n",
    "\n",
    "    pool1 = GlobalAveragePooling2D()(block_input)\n",
    "    flat = Reshape((1, 1, num_filters))(pool1)\n",
    "    dense1 = Dense(num_filters//ratio, activation='relu')(flat)\n",
    "    dense2 = Dense(num_filters, activation='sigmoid')(dense1)\n",
    "    scale = multiply([block_input, dense2])\n",
    "    \n",
    "    return scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SE block V2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excite_block(tensor, ratio=16):\n",
    "    init = tensor\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    filters = init._keras_shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se = Permute((3, 1, 2))(se)\n",
    "\n",
    "    x = multiply([init, se])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters3, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    \n",
    "#     add the se_block\n",
    "    x = squeeze_excite_block(x)\n",
    "    # add the cbam block\n",
    "#    x = cbam_block(x)\n",
    "    \n",
    "    \n",
    "    x = add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor,\n",
    "               kernel_size,\n",
    "               filters,\n",
    "               stage,\n",
    "               block,\n",
    "               strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        strides: Strides for the first conv layer in the block.\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3,\n",
    "    the first conv layer at main path is with strides=(2, 2)\n",
    "    And the shortcut should have strides=(2, 2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    \n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = 3\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "    \n",
    "    \n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides,\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters3, (1, 1),\n",
    "                      kernel_initializer='he_normal',\n",
    "                      name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    \n",
    "    \n",
    "    # add the se_block\n",
    "    x = squeeze_excite_block(x)\n",
    "    # add the cbam block\n",
    "#    x = cbam_block(x)\n",
    "    \n",
    "    \n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler(Layer):\n",
    "    def __init__(self, tau=0.5, **kwargs):\n",
    "        super(Scaler, self).__init__(**kwargs)\n",
    "        self.tau = tau\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(Scaler, self).build(input_shape)\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale', shape=(input_shape[-1],), initializer='zeros'\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, mode='positive'):\n",
    "        if mode == 'positive':\n",
    "            scale = self.tau + (1 - self.tau) * K.sigmoid(self.scale)\n",
    "        \n",
    "        else:\n",
    "            scale = (1 - self.tau) * K.sigmoid(-self.scale)\n",
    "        \n",
    "        return inputs * K.sqrt(scale)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'tau': self.tau}\n",
    "        base_config = super(Scaler, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    data_format = K.image_data_format()\n",
    "    \n",
    "    if data_format == 'channels_last':\n",
    "        # 'RGB'->'BGR'\n",
    "        if img.ndim == 3:\n",
    "            img = img[::-1, ...]\n",
    "        else:\n",
    "            img = img[:, ::-1, ...]\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        # 'RGB'->'BGR'\n",
    "        img = img[..., ::-1]\n",
    "    \n",
    "    mean = [103.939, 116.779, 123.68]\n",
    "    std = None\n",
    "    \n",
    "    # Zero-center by mean pixel\n",
    "    if data_format == 'channels_first':\n",
    "        if img.ndim == 3:\n",
    "            img[0, :, :] -= mean[0]\n",
    "            img[1, :, :] -= mean[1]\n",
    "            img[2, :, :] -= mean[2]\n",
    "\n",
    "\n",
    "            if std is not None:\n",
    "                img[0, :, :] /= std[0]\n",
    "                img[1, :, :] /= std[1]\n",
    "                img[2, :, :] /= std[2]\n",
    "        else:\n",
    "            img[:, 0, :, :] -= mean[0]\n",
    "            img[:, 1, :, :] -= mean[1]\n",
    "            img[:, 2, :, :] -= mean[2]\n",
    "            if std is not None:\n",
    "                img[:, 0, :, :] /= std[0]\n",
    "                img[:, 1, :, :] /= std[1]\n",
    "                img[:, 2, :, :] /= std[2]\n",
    "    else:\n",
    "        img[..., 0] -= mean[0]\n",
    "        img[..., 1] -= mean[1]\n",
    "        img[..., 2] -= mean[2]\n",
    "\n",
    "\n",
    "        if std is not None:\n",
    "            img[..., 0] /= std[0]\n",
    "            img[..., 1] /= std[1]\n",
    "            img[..., 2] /= std[2]\n",
    "\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator_multiple(generator,dir1, batch_size, IMG_SHAPE):\n",
    "    genX1 = generator.flow_from_directory(\n",
    "                                        batch_size=batch_size,\n",
    "                                        directory=dir1,\n",
    "                                        shuffle=True,\n",
    "                                        color_mode=\"rgb\",\n",
    "                                        target_size=(IMG_SHAPE,IMG_SHAPE),\n",
    "                                        class_mode='categorical',\n",
    "                                        seed = 42)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            attr_array = []\n",
    "            for i in range(X1i[1].shape[0]):\n",
    "                idx = np.where(X1i[1][i])[0][0]\n",
    "                attr_array.append(attr_list[idx])\n",
    "            \n",
    "            yield [X1i[0], np.array(attr_array)], X1i[1] #Yield both images and their mutual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "train_data_gen = generate_generator_multiple(image_gen, train_dir, batch_size, IMG_SHAPE)\n",
    "val_data_gen = generate_generator_multiple(image_gen, val_dir, batch_size, IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58176 images belonging to 40 classes.\n",
      "Found 15872 images belonging to 40 classes.\n"
     ]
    }
   ],
   "source": [
    "image_size = IMG_SHAPE\n",
    "tmp_train_gen = image_gen.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=train_dir,\n",
    "    color_mode=\"rgb\",\n",
    "    target_size=(image_size,image_size),\n",
    "    class_mode='sparse',\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "\n",
    "tmp_val_gen = image_gen.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory=val_dir,\n",
    "    target_size=(image_size, image_size),\n",
    "    class_mode='sparse',\n",
    "    color_mode=\"rgb\",\n",
    "    seed = 42\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "           'balanced',\n",
    "            np.unique(tmp_train_gen.classes), \n",
    "            tmp_train_gen.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_in = Input(shape=class_attr_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    bn_axis = 3\n",
    "else:\n",
    "    bn_axis = 1\n",
    "\n",
    "x_in = Input(shape=(IMG_SHAPE, IMG_SHAPE, 3))\n",
    "x = x_in\n",
    "\n",
    "x = ZeroPadding2D(padding=(3, 3), name='conv1_pad')(x)\n",
    "x = Conv2D(64, (7, 7),\n",
    "                  strides=(2, 2),\n",
    "                  padding='valid',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  name='conv1')(x)\n",
    "x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# add the se_block\n",
    "# x = se_block(x, num_filters=filters3)\n",
    "# add the cbam block\n",
    "x = squeeze_excite_block(x)\n",
    "\n",
    "\n",
    "x = ZeroPadding2D(padding=(1, 1), name='pool1_pad')(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "\n",
    "# add the cbam block\n",
    "x = squeeze_excite_block(x)\n",
    "GAP_x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "\n",
    "# classifier classes\n",
    "t = Dense(1024, activation='relu')(GAP_x)\n",
    "# add a classifier\n",
    "predictions = Dense(seen_class_num, activation='softmax', name='predict')(t)\n",
    "\n",
    "# generate ce\n",
    "x = Dense(2048, activation='relu')(GAP_x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "z_mean = Dense(class_attr_dim)(x)\n",
    "z_log_var = Dense(class_attr_dim)(x)\n",
    "\n",
    "scaler = Scaler()\n",
    "z_mean = scaler(z_mean, mode='positive')\n",
    "z_log_var = scaler(z_log_var, mode='negative')\n",
    "\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    u = K.random_normal(shape=K.shape(z_mean))\n",
    "    return z_mean + K.exp(z_log_var / 2) * u\n",
    "\n",
    "\n",
    "z_samples = Lambda(sampling)([z_mean, z_log_var])\n",
    "encoder = Model(x_in, z_samples) \n",
    "\n",
    "prior_kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean - y_in) - K.exp(z_log_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = Model([x_in, y_in],[predictions])\n",
    "model_train.load_weights('/home/uscc/.keras/models/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 訓練model\n",
    "\n",
    "model_train.add_loss(gamma * prior_kl_loss)\n",
    "\n",
    "losses = {\"predict\": \"categorical_crossentropy\"}\n",
    "lossWeights = {\"predict\":alpha}\n",
    "\n",
    "model_train.compile(optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "                    loss=losses,\n",
    "                    loss_weights=lossWeights,\n",
    "                    metrics=['accuracy']\n",
    "                   )\n",
    "\n",
    "model_train.metrics_tensors.append(gamma * prior_kl_loss)\n",
    "model_train.metrics_names.append(\"kl_loss\")\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10,verbose=1)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=5, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.1, \n",
    "                                            mode='auto')\n",
    "\n",
    "\n",
    "STEP_SIZE_TRAIN=tmp_train_gen.n//tmp_train_gen.batch_size\n",
    "STEP_SIZE_VALID=tmp_val_gen.n//tmp_val_gen.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_train.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/uscc/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/50\n",
      "Found 15872 images belonging to 40 classes.\n",
      "Found 58176 images belonging to 40 classes.\n",
      "909/909 [==============================] - 1521s 2s/step - loss: 3.1594 - acc: 0.5649 - kl_loss: 2.3912 - val_loss: 1.6648 - val_acc: 0.6038 - val_kl_loss: 0.9682\n",
      "Epoch 2/50\n",
      "909/909 [==============================] - 1388s 2s/step - loss: 2.3106 - acc: 0.8578 - kl_loss: 2.0750 - val_loss: 1.5440 - val_acc: 0.6987 - val_kl_loss: 0.9698\n",
      "Epoch 3/50\n",
      "909/909 [==============================] - 1297s 1s/step - loss: 2.0008 - acc: 0.9283 - kl_loss: 1.8833 - val_loss: 1.6395 - val_acc: 0.6562 - val_kl_loss: 0.9466\n",
      "Epoch 4/50\n",
      "909/909 [==============================] - 1290s 1s/step - loss: 1.8293 - acc: 0.9588 - kl_loss: 1.7626 - val_loss: 1.5111 - val_acc: 0.7366 - val_kl_loss: 0.9477\n",
      "Epoch 5/50\n",
      "909/909 [==============================] - 1305s 1s/step - loss: 1.7338 - acc: 0.9715 - kl_loss: 1.6885 - val_loss: 1.5638 - val_acc: 0.7275 - val_kl_loss: 0.9408\n",
      "Epoch 6/50\n",
      "909/909 [==============================] - 1298s 1s/step - loss: 1.6634 - acc: 0.9811 - kl_loss: 1.6327 - val_loss: 1.5381 - val_acc: 0.7434 - val_kl_loss: 0.9475\n",
      "Epoch 7/50\n",
      "909/909 [==============================] - 1282s 1s/step - loss: 1.6211 - acc: 0.9842 - kl_loss: 1.5953 - val_loss: 1.5559 - val_acc: 0.7324 - val_kl_loss: 0.9299\n",
      "Epoch 8/50\n",
      "909/909 [==============================] - 1293s 1s/step - loss: 1.5866 - acc: 0.9868 - kl_loss: 1.5658 - val_loss: 1.5268 - val_acc: 0.7518 - val_kl_loss: 0.9209\n",
      "Epoch 9/50\n",
      "909/909 [==============================] - 1291s 1s/step - loss: 1.5567 - acc: 0.9895 - kl_loss: 1.5397 - val_loss: 1.7056 - val_acc: 0.7014 - val_kl_loss: 0.9308\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 10/50\n",
      "909/909 [==============================] - 1298s 1s/step - loss: 1.5250 - acc: 0.9959 - kl_loss: 1.5179 - val_loss: 1.4101 - val_acc: 0.7911 - val_kl_loss: 0.9114\n",
      "Epoch 11/50\n",
      "909/909 [==============================] - 1288s 1s/step - loss: 1.5041 - acc: 0.9988 - kl_loss: 1.5017 - val_loss: 1.4021 - val_acc: 0.7945 - val_kl_loss: 0.9085\n",
      "Epoch 12/50\n",
      "909/909 [==============================] - 1291s 1s/step - loss: 1.4973 - acc: 0.9994 - kl_loss: 1.4957 - val_loss: 1.4017 - val_acc: 0.7942 - val_kl_loss: 0.9075\n",
      "Epoch 13/50\n",
      "909/909 [==============================] - 1288s 1s/step - loss: 1.4894 - acc: 0.9994 - kl_loss: 1.4880 - val_loss: 1.3997 - val_acc: 0.7958 - val_kl_loss: 0.9059\n",
      "Epoch 14/50\n",
      "909/909 [==============================] - 1287s 1s/step - loss: 1.4878 - acc: 0.9993 - kl_loss: 1.4865 - val_loss: 1.3958 - val_acc: 0.7950 - val_kl_loss: 0.9033\n",
      "Epoch 15/50\n",
      "909/909 [==============================] - 1287s 1s/step - loss: 1.4830 - acc: 0.9995 - kl_loss: 1.4820 - val_loss: 1.3948 - val_acc: 0.7973 - val_kl_loss: 0.9026\n",
      "Epoch 16/50\n",
      "909/909 [==============================] - 1289s 1s/step - loss: 1.4800 - acc: 0.9995 - kl_loss: 1.4789 - val_loss: 1.3998 - val_acc: 0.7958 - val_kl_loss: 0.9020\n",
      "Epoch 17/50\n",
      "909/909 [==============================] - 1284s 1s/step - loss: 1.4787 - acc: 0.9996 - kl_loss: 1.4776 - val_loss: 1.3938 - val_acc: 0.7957 - val_kl_loss: 0.9007\n",
      "Epoch 18/50\n",
      "909/909 [==============================] - 1282s 1s/step - loss: 1.4749 - acc: 0.9995 - kl_loss: 1.4739 - val_loss: 1.3960 - val_acc: 0.7957 - val_kl_loss: 0.8988\n",
      "Epoch 19/50\n",
      "909/909 [==============================] - 1296s 1s/step - loss: 1.4738 - acc: 0.9996 - kl_loss: 1.4730 - val_loss: 1.3946 - val_acc: 0.7954 - val_kl_loss: 0.8977\n",
      "Epoch 20/50\n",
      "909/909 [==============================] - 1303s 1s/step - loss: 1.4684 - acc: 0.9995 - kl_loss: 1.4673 - val_loss: 1.3964 - val_acc: 0.7945 - val_kl_loss: 0.8964\n",
      "Epoch 21/50\n",
      "909/909 [==============================] - 1293s 1s/step - loss: 1.4673 - acc: 0.9995 - kl_loss: 1.4664 - val_loss: 1.3959 - val_acc: 0.7955 - val_kl_loss: 0.8957\n",
      "Epoch 22/50\n",
      "909/909 [==============================] - 1296s 1s/step - loss: 1.4631 - acc: 0.9996 - kl_loss: 1.4623 - val_loss: 1.3909 - val_acc: 0.7959 - val_kl_loss: 0.8938\n",
      "Epoch 23/50\n",
      "909/909 [==============================] - 1302s 1s/step - loss: 1.4633 - acc: 0.9996 - kl_loss: 1.4624 - val_loss: 1.3948 - val_acc: 0.7954 - val_kl_loss: 0.8940\n",
      "Epoch 24/50\n",
      "909/909 [==============================] - 1300s 1s/step - loss: 1.4600 - acc: 0.9996 - kl_loss: 1.4592 - val_loss: 1.3965 - val_acc: 0.7951 - val_kl_loss: 0.8925\n",
      "Epoch 25/50\n",
      "909/909 [==============================] - 1307s 1s/step - loss: 1.4584 - acc: 0.9995 - kl_loss: 1.4575 - val_loss: 1.3935 - val_acc: 0.7964 - val_kl_loss: 0.8906\n",
      "Epoch 26/50\n",
      "909/909 [==============================] - 1303s 1s/step - loss: 1.4577 - acc: 0.9996 - kl_loss: 1.4568 - val_loss: 1.3905 - val_acc: 0.7956 - val_kl_loss: 0.8897\n",
      "Epoch 27/50\n",
      "909/909 [==============================] - 1301s 1s/step - loss: 1.4549 - acc: 0.9995 - kl_loss: 1.4540 - val_loss: 1.3998 - val_acc: 0.7939 - val_kl_loss: 0.8906\n",
      "Epoch 28/50\n",
      "909/909 [==============================] - 1297s 1s/step - loss: 1.4521 - acc: 0.9996 - kl_loss: 1.4512 - val_loss: 1.3956 - val_acc: 0.7950 - val_kl_loss: 0.8884\n",
      "Epoch 29/50\n",
      "909/909 [==============================] - 1306s 1s/step - loss: 1.4517 - acc: 0.9996 - kl_loss: 1.4507 - val_loss: 1.3897 - val_acc: 0.7947 - val_kl_loss: 0.8867\n",
      "Epoch 30/50\n",
      "909/909 [==============================] - 1302s 1s/step - loss: 1.4498 - acc: 0.9996 - kl_loss: 1.4490 - val_loss: 1.3997 - val_acc: 0.7917 - val_kl_loss: 0.8871\n",
      "Epoch 31/50\n",
      "909/909 [==============================] - 1292s 1s/step - loss: 1.4479 - acc: 0.9995 - kl_loss: 1.4470 - val_loss: 1.3965 - val_acc: 0.7957 - val_kl_loss: 0.8856\n",
      "Epoch 32/50\n",
      "909/909 [==============================] - 1294s 1s/step - loss: 1.4447 - acc: 0.9995 - kl_loss: 1.4438 - val_loss: 1.3929 - val_acc: 0.7951 - val_kl_loss: 0.8844\n",
      "Epoch 33/50\n",
      "909/909 [==============================] - 1299s 1s/step - loss: 1.4436 - acc: 0.9997 - kl_loss: 1.4428 - val_loss: 1.3934 - val_acc: 0.7949 - val_kl_loss: 0.8831\n",
      "Epoch 34/50\n",
      "909/909 [==============================] - 1299s 1s/step - loss: 1.4413 - acc: 0.9996 - kl_loss: 1.4405 - val_loss: 1.3951 - val_acc: 0.7958 - val_kl_loss: 0.8830\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 35/50\n",
      "909/909 [==============================] - 1293s 1s/step - loss: 1.4419 - acc: 0.9997 - kl_loss: 1.4412 - val_loss: 1.3941 - val_acc: 0.7950 - val_kl_loss: 0.8823\n",
      "Epoch 36/50\n",
      "909/909 [==============================] - 1292s 1s/step - loss: 1.4408 - acc: 0.9996 - kl_loss: 1.4400 - val_loss: 1.3948 - val_acc: 0.7948 - val_kl_loss: 0.8822\n",
      "Epoch 37/50\n",
      "909/909 [==============================] - 1291s 1s/step - loss: 1.4409 - acc: 0.9996 - kl_loss: 1.4402 - val_loss: 1.3967 - val_acc: 0.7940 - val_kl_loss: 0.8823\n",
      "Epoch 38/50\n",
      "909/909 [==============================] - 1301s 1s/step - loss: 1.4404 - acc: 0.9996 - kl_loss: 1.4396 - val_loss: 1.4003 - val_acc: 0.7923 - val_kl_loss: 0.8826\n",
      "Epoch 39/50\n",
      "909/909 [==============================] - 1296s 1s/step - loss: 1.4406 - acc: 0.9997 - kl_loss: 1.4399 - val_loss: 1.3975 - val_acc: 0.7932 - val_kl_loss: 0.8825\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 00039: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ae6877ac8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit_generator(\n",
    "                    train_data_gen,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    epochs = epochs,\n",
    "                    validation_data=val_data_gen,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "#                     class_weight=class_weights,\n",
    "                    callbacks = [early_stopping, learning_rate_reduction]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('./model/{}/encoder_{}.h5'.format(dataset,'without_infomax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = load_model('./model/{}/encoder_{}.h5'.format(dataset,'without_infomax'), custom_objects={'Scaler': Scaler})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [103.939, 116.779, 123.68]\n",
    "\n",
    "# 随机选一张图片，输出最相近的图片\n",
    "# 可以选用欧氏距离或者cos值\n",
    "def sample_knn(path, prefix):\n",
    "    n = 10\n",
    "    topn = 10\n",
    "    figure1 = np.zeros((IMG_SHAPE*n, IMG_SHAPE*topn, 3))\n",
    "    figure2 = np.zeros((IMG_SHAPE*n, IMG_SHAPE*topn, 3))\n",
    "    zs_ = zs / (zs**2).sum(1, keepdims=True)**0.5\n",
    "#     zs_ = zs\n",
    "    for i in range(n):\n",
    "        one = np.random.choice(len(x_test))\n",
    "        idxs = ((zs**2).sum(1) + (zs[one]**2).sum() - 2 * np.dot(zs, zs[one])).argsort()[:topn]\n",
    "        for j,k in enumerate(idxs):\n",
    "            digit = x_test[k]\n",
    "            figure1[i*IMG_SHAPE: (i+1)*IMG_SHAPE,\n",
    "                   j*IMG_SHAPE: (j+1)*IMG_SHAPE] = digit\n",
    "        idxs = np.dot(zs_, zs_[one]).argsort()[-n:][::-1]\n",
    "        for j,k in enumerate(idxs):\n",
    "            digit = x_test[k]\n",
    "            figure2[i*IMG_SHAPE: (i+1)*IMG_SHAPE,\n",
    "                   j*IMG_SHAPE: (j+1)*IMG_SHAPE] = digit\n",
    "    \n",
    "    figure1[..., 0] += mean[0]\n",
    "    figure1[..., 1] += mean[1]\n",
    "    figure1[..., 2] += mean[2]\n",
    "    figure1 = figure1[::-1, ...]\n",
    "    figure1 = np.clip(figure1, 0, 255)\n",
    "    \n",
    "    figure2[..., 0] += mean[0]\n",
    "    figure2[..., 1] += mean[1]\n",
    "    figure2[..., 2] += mean[2]\n",
    "    figure2 = figure2[::-1, ...]\n",
    "    figure2 = np.clip(figure1, 0, 255)\n",
    "    imageio.imwrite(path+'_l2_'+ prefix + '.png', figure1)\n",
    "    imageio.imwrite(path+'_cos_'+ prefix + '.png', figure2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9938 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "tmp_test_gen = image_gen.flow_from_directory(\n",
    "    batch_size=batch_size,\n",
    "    directory='./data/{}/IMG/test'.format(dataset),\n",
    "    target_size=(image_size, image_size),\n",
    "    class_mode='sparse',\n",
    "    color_mode=\"rgb\",\n",
    "    seed = 42\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 128, 3)\n",
      "(128, 128, 128, 3)\n",
      "(192, 128, 128, 3)\n",
      "(256, 128, 128, 3)\n",
      "(320, 128, 128, 3)\n",
      "(384, 128, 128, 3)\n",
      "(448, 128, 128, 3)\n",
      "(512, 128, 128, 3)\n",
      "(576, 128, 128, 3)\n",
      "(640, 128, 128, 3)\n",
      "(704, 128, 128, 3)\n",
      "(768, 128, 128, 3)\n",
      "(832, 128, 128, 3)\n",
      "(896, 128, 128, 3)\n",
      "(960, 128, 128, 3)\n",
      "(1024, 128, 128, 3)\n",
      "(1088, 128, 128, 3)\n",
      "(1152, 128, 128, 3)\n",
      "(1216, 128, 128, 3)\n",
      "(1280, 128, 128, 3)\n",
      "(1344, 128, 128, 3)\n",
      "(1408, 128, 128, 3)\n",
      "(1472, 128, 128, 3)\n",
      "(1536, 128, 128, 3)\n",
      "(1600, 128, 128, 3)\n",
      "(1664, 128, 128, 3)\n",
      "(1728, 128, 128, 3)\n",
      "(1792, 128, 128, 3)\n",
      "(1856, 128, 128, 3)\n",
      "(1920, 128, 128, 3)\n",
      "(1984, 128, 128, 3)\n",
      "(2048, 128, 128, 3)\n",
      "(2112, 128, 128, 3)\n",
      "(2176, 128, 128, 3)\n",
      "(2240, 128, 128, 3)\n",
      "(2304, 128, 128, 3)\n",
      "(2368, 128, 128, 3)\n",
      "(2432, 128, 128, 3)\n",
      "(2496, 128, 128, 3)\n",
      "(2560, 128, 128, 3)\n",
      "(2624, 128, 128, 3)\n",
      "(2688, 128, 128, 3)\n",
      "(2752, 128, 128, 3)\n",
      "(2816, 128, 128, 3)\n",
      "(2880, 128, 128, 3)\n",
      "(2944, 128, 128, 3)\n",
      "(3008, 128, 128, 3)\n",
      "(3072, 128, 128, 3)\n",
      "(3136, 128, 128, 3)\n",
      "(3200, 128, 128, 3)\n",
      "(3264, 128, 128, 3)\n",
      "(3328, 128, 128, 3)\n",
      "(3392, 128, 128, 3)\n",
      "(3456, 128, 128, 3)\n",
      "(3520, 128, 128, 3)\n",
      "(3584, 128, 128, 3)\n",
      "(3648, 128, 128, 3)\n",
      "(3712, 128, 128, 3)\n",
      "(3776, 128, 128, 3)\n",
      "(3840, 128, 128, 3)\n",
      "(3904, 128, 128, 3)\n",
      "(3968, 128, 128, 3)\n",
      "(4032, 128, 128, 3)\n",
      "(4096, 128, 128, 3)\n",
      "(4160, 128, 128, 3)\n",
      "(4224, 128, 128, 3)\n",
      "(4288, 128, 128, 3)\n",
      "(4352, 128, 128, 3)\n",
      "(4416, 128, 128, 3)\n",
      "(4480, 128, 128, 3)\n",
      "(4544, 128, 128, 3)\n",
      "(4608, 128, 128, 3)\n",
      "(4672, 128, 128, 3)\n",
      "(4736, 128, 128, 3)\n",
      "(4800, 128, 128, 3)\n",
      "(4864, 128, 128, 3)\n",
      "(4928, 128, 128, 3)\n",
      "(4992, 128, 128, 3)\n",
      "(5056, 128, 128, 3)\n",
      "(5120, 128, 128, 3)\n",
      "(5184, 128, 128, 3)\n",
      "(5248, 128, 128, 3)\n",
      "(5312, 128, 128, 3)\n",
      "(5376, 128, 128, 3)\n",
      "(5440, 128, 128, 3)\n",
      "(5504, 128, 128, 3)\n",
      "(5568, 128, 128, 3)\n",
      "(5632, 128, 128, 3)\n",
      "(5696, 128, 128, 3)\n",
      "(5760, 128, 128, 3)\n",
      "(5824, 128, 128, 3)\n",
      "(5888, 128, 128, 3)\n",
      "(5952, 128, 128, 3)\n",
      "(6016, 128, 128, 3)\n",
      "(6080, 128, 128, 3)\n",
      "(6144, 128, 128, 3)\n",
      "(6208, 128, 128, 3)\n",
      "(6272, 128, 128, 3)\n",
      "(6336, 128, 128, 3)\n",
      "(6400, 128, 128, 3)\n",
      "(6464, 128, 128, 3)\n",
      "(6528, 128, 128, 3)\n",
      "(6592, 128, 128, 3)\n",
      "(6656, 128, 128, 3)\n",
      "(6720, 128, 128, 3)\n",
      "(6784, 128, 128, 3)\n",
      "(6848, 128, 128, 3)\n",
      "(6912, 128, 128, 3)\n",
      "(6976, 128, 128, 3)\n",
      "(7040, 128, 128, 3)\n",
      "(7104, 128, 128, 3)\n",
      "(7168, 128, 128, 3)\n",
      "(7232, 128, 128, 3)\n",
      "(7296, 128, 128, 3)\n",
      "(7360, 128, 128, 3)\n",
      "(7424, 128, 128, 3)\n",
      "(7488, 128, 128, 3)\n",
      "(7552, 128, 128, 3)\n",
      "(7616, 128, 128, 3)\n",
      "(7680, 128, 128, 3)\n",
      "(7744, 128, 128, 3)\n",
      "(7808, 128, 128, 3)\n",
      "(7872, 128, 128, 3)\n",
      "(7936, 128, 128, 3)\n",
      "(8000, 128, 128, 3)\n",
      "(8064, 128, 128, 3)\n",
      "(8128, 128, 128, 3)\n",
      "(8192, 128, 128, 3)\n",
      "(8256, 128, 128, 3)\n",
      "(8320, 128, 128, 3)\n",
      "(8384, 128, 128, 3)\n",
      "(8448, 128, 128, 3)\n",
      "(8512, 128, 128, 3)\n",
      "(8576, 128, 128, 3)\n",
      "(8640, 128, 128, 3)\n",
      "(8704, 128, 128, 3)\n",
      "(8768, 128, 128, 3)\n",
      "(8832, 128, 128, 3)\n",
      "(8896, 128, 128, 3)\n",
      "(8960, 128, 128, 3)\n",
      "(9024, 128, 128, 3)\n",
      "(9088, 128, 128, 3)\n",
      "(9152, 128, 128, 3)\n",
      "(9216, 128, 128, 3)\n",
      "(9280, 128, 128, 3)\n",
      "(9344, 128, 128, 3)\n",
      "(9408, 128, 128, 3)\n",
      "(9472, 128, 128, 3)\n",
      "(9536, 128, 128, 3)\n",
      "(9600, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array([]).reshape(0,IMG_SHAPE,IMG_SHAPE,3)\n",
    "x = 0\n",
    "for data in tmp_test_gen:\n",
    "    x_test = np.concatenate((x_test, data[0]))\n",
    "    x += 1\n",
    "    print(x_test.shape)\n",
    "    if x == 150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600/9600 [==============================] - 27s 3ms/step\n",
      "-0.0064043254\n",
      "1.0707442\n"
     ]
    }
   ],
   "source": [
    "# 输出编码器的特征\n",
    "zs = encoder.predict(x_test, verbose=True)\n",
    "print(zs.mean()) # 查看均值\n",
    "print(zs.std()) # 查看方差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0.0, 255.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "sample_knn('./library_Pic/', 'unseen_without_deepinfo_with_categrocial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([]).reshape(0,IMG_SHAPE,IMG_SHAPE,3)\n",
    "\n",
    "x = 0\n",
    "for data in tmp_val_gen:\n",
    "    x_test = np.concatenate((x_test, data[0]))\n",
    "    x += 1\n",
    "    print(x_test.shape)\n",
    "    if x == 150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出编码器的特征\n",
    "zs = encoder.predict(x_test, verbose=True)\n",
    "print(zs.mean()) # 查看均值\n",
    "print(zs.std()) # 查看方差\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_knn('./library_Pic/', 'seen_without_deepinfo_with_categrocial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 觀察seen attr分佈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attr_list_cms = []\n",
    "classname = pd.read_csv('./data/{}/classes.txt'.format(dataset), header=None, sep='\\t')\n",
    "\n",
    "for k ,v in tmp_train_gen.class_indices.items():\n",
    "    idx = np.where(classname[1] == k)\n",
    "    train_attr_list_cms.append(attr_list[idx[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum 40 class attributes\n",
    "sum_attr = [[] for i in range(seen_class_num)]\n",
    "real_attr = [[] for i in range(seen_class_num)]\n",
    "count_class = [0 for i in range(seen_class_num)]\n",
    "\n",
    "N = tmp_train_gen.n - 100\n",
    "s = 0 \n",
    "for data, label in tmp_train_gen:\n",
    "    ce = encoder.predict(data)\n",
    "    s += batch_size\n",
    "    print(s, ' ', N)\n",
    "    if s > N:\n",
    "        break\n",
    "    for idx in range(batch_size):\n",
    "        l = int(label[idx])\n",
    "        if sum_attr[l] == []:\n",
    "            sum_attr[l] = ce[idx]\n",
    "            real_attr[l] = train_attr_list_cms[l]\n",
    "        else:    \n",
    "            sum_attr[l] += ce[idx]\n",
    "    \n",
    "        count_class[l] += 1\n",
    "# averge\n",
    "for i in range(seen_class_num):\n",
    "    sum_attr[i] = sum_attr[i] / count_class[i]\n",
    "\n",
    "sum_attr = np.array(sum_attr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_class = []\n",
    "for k,v in tmp_train_gen.class_indices.items():\n",
    "    seen_class.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "attributes_name = pd.read_csv('./data/{}/predicates.txt'.format(dataset),header=None,sep='\\t')\n",
    "\n",
    "\n",
    "for i in range(seen_class_num):\n",
    "    diff = round(np.sum(np.abs(sum_attr[i] - real_attr[i])) / len(real_attr[i]),6)\n",
    "    plt.figure(figsize=(40,10))\n",
    "    plt.bar(attributes_name[1],height=sum_attr[i],align='edge',label = 'LearnedCE',width = 0.25)\n",
    "    plt.bar(attributes_name[1],height=real_attr[i],align='edge',label = 'RealCE',width=-0.25)\n",
    "    plt.legend(fontsize=15) #要使用label要加這行\n",
    "    plt.xlabel('Attributes',fontsize=30) # 設定x軸標題\n",
    "    plt.xticks(fontsize=20,rotation='vertical')\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.title('Class : ' + seen_class[i] +',diff : '+ str(diff),fontsize=40) # 設定圖表標題\n",
    "#     plt.savefig('./data/{}/mat/{}_{}/seen/{}.jpg'.format(dataset,feature_type,attr_type,seen_class[i]))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('3.6.8': pyenv)",
   "language": "python",
   "name": "python36864bit368pyenvd95b829e87414dda87f73a84dcd10ea0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
